{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm, tqdm_notebook\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport json\nprint(os.listdir(\"../input\"))\ninput_meta_dir = '../input/petfinder-adoption-prediction/'\ntrain_images_data_dir = '../input/petfinder-adoption-prediction/train_images'\ntest_images_data_dir = '../input/petfinder-adoption-prediction/test_images'\n#input_meta_dir = '../input/'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "meta_train = pd.read_csv(input_meta_dir + 'train/train.csv')\nmeta_test = pd.read_csv(input_meta_dir + 'test/test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b736a7dad7ddd80719bf512d30e55afe15ab27e"
      },
      "cell_type": "code",
      "source": "print(meta_train.shape)\nprint(meta_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b4bad190c9a740431454b18e88788064d7d303b"
      },
      "cell_type": "code",
      "source": "def load_sentiment_dataframe(split_type='train'):\n    sentiment_split = '{split_type}_sentiment/'.format(split_type=split_type)\n    train_sentiment = []\n    for filename in os.listdir(input_meta_dir + sentiment_split):\n        with open(input_meta_dir + sentiment_split + filename, 'r') as json_file:    \n            data = json.load(json_file)\n            info_to_keep = data['documentSentiment'] # e.g. {'magnitude': 2.1, 'score': 0.4}\n            info_to_keep['Language'] = data['language']\n            info_to_keep['PetID'] = filename.split('.')[0]\n            train_sentiment.append(info_to_keep)\n\n    train_sentiment_df = pd.DataFrame(train_sentiment)\n    return train_sentiment_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6b8016dc1baa1ae41f72f2ab71525e09141873b"
      },
      "cell_type": "code",
      "source": "train_sentiment = load_sentiment_dataframe()\ntest_sentiment = load_sentiment_dataframe(split_type='test')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5ae8659e2011cfa07388ca812f83b703adad9dc"
      },
      "cell_type": "code",
      "source": "meta_train = meta_train.merge(train_sentiment, on='PetID', how='left')\nmeta_test = meta_test.merge(test_sentiment, on='PetID', how='left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ecbf24db42586811fec976edb246f8d4cdd22a32"
      },
      "cell_type": "code",
      "source": "data = meta_train.append(meta_test, sort=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3bbe8a8a637d337015a2a48feb1eb25f3049dcc6"
      },
      "cell_type": "code",
      "source": "data.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6187a571b5d1aad1be0c8e38a9cea788ea341ad"
      },
      "cell_type": "code",
      "source": "def get_labels_map(label_type):\n    labels = pd.read_csv(input_meta_dir + label_type.lower() + '_labels.csv')\n    labels_map = dict(zip(labels['{}ID'.format(label_type)], labels['{}Name'.format(label_type)]))\n    return labels_map\n\nstate_map = get_labels_map('State')\nbreed_map = get_labels_map('Breed')\ncolor_map = get_labels_map('Color')\n\ndata.loc[:, 'Type'] = data.Type.map({1:'Dog', 2:'Cat'})\ndata.loc[:, 'Breed1'] = data.Breed1.map(breed_map)\ndata.loc[:, 'Breed2'] = data.Breed2.map(breed_map)\ndata.loc[:, 'Color1'] = data.Color1.map(color_map)\ndata.loc[:, 'Color2'] = data.Color2.map(color_map)\ndata.loc[:, 'Color3'] = data.Color3.map(color_map)\ndata.loc[:, 'State'] = data.State.map(state_map)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22d33726c54bf1b8e3ec1a678d5f01fe2282ef98"
      },
      "cell_type": "code",
      "source": "def get_impute_dict(data, var_name, top_n):\n    impute_dict = {x:x for x in data[var_name].value_counts()[:top_n].index.values}\n    impute_dict.update({'Missing':'Missing'})\n    return impute_dict",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7334dd5eb1a60510d1d373c0e91d1f5722739176"
      },
      "cell_type": "code",
      "source": "def clean_data(data):\n    data.Name.fillna('no name', inplace=True)\n    data.loc[:, 'NoName'] = data.Name.str.lower().str.contains('no name').astype(int)\n    data.loc[:, 'NameWordLength'] = data.Name.apply(lambda x: len(x.split(' ')))\n    data.Description.fillna('None', inplace=True)\n    data.loc[:, 'NameInDescription'] = data.apply(lambda record: record.Name.lower() in record.Description.lower(), axis=1).astype(int)\n    data.loc[:, 'DescriptionWordLength'] = data.Description.apply(lambda x: len(x.split(' ')))\n    #bins = np.array([0, 10, 25, 50, 100, 150, 200])\n    #data.loc[:, 'DescriptionWordLength'] = np.digitize(data.DescriptionWordLength.values, bins)\n    #data.loc[:, 'Breed1'] = data.Breed1.fillna('Missing').map(get_impute_dict(data, 'Breed1', 10)).fillna('Other')\n    #data.loc[:, 'Breed2'] = data.Breed2.fillna('Missing').map(get_impute_dict(data, 'Breed2', 10)).fillna('Other')\n    data.loc[:, 'Breed1'] = data.Breed1.map(get_impute_dict(data, 'Breed1', 10)).fillna('Other')\n    data.loc[:, 'Breed2'] = data.Breed2.map(get_impute_dict(data, 'Breed2', 10)).fillna('Other')\n    data.loc[:, 'RescuerCount'] = data.groupby(['RescuerID'])['Type'].transform('count') \n    return data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "97ac7cba4f504bcffd23074e4a6a7a182ffad50e"
      },
      "cell_type": "code",
      "source": "data = clean_data (data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6cfc8ebe2ff01cda48b392cde8e2953ad6a48725"
      },
      "cell_type": "code",
      "source": "data.tail(2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fdca929e980fd2039ea3f254d9fa862541e2eb72"
      },
      "cell_type": "code",
      "source": "# Use this for target-encoding - may be done better\n# Modified to include test set as well\n\ndef calc_smooth_mean(df, by, on, m):\n    # df - input pandas dataframe\n    # Compute the global mean\n    mean = df[on].mean()\n\n    # Compute the number of values and the mean of each group\n    agg = df.groupby(by)[on].agg(['count', 'mean'])\n    counts = agg['count']\n    means = agg['mean']\n\n    # Compute the \"smoothed\" means\n    smooth = (counts * means + m * mean) / (counts + m)\n\n    # Replace each value by the according smoothed mean\n    #return smooth\n    return df[by].map(smooth)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c61acdca448c5ad5a932bdcf9e2a304953a3bec3"
      },
      "cell_type": "code",
      "source": "# TE = target encoding\nte_weight = 10\ndata['Breed1'] = calc_smooth_mean(data, by='Breed1', on='AdoptionSpeed', m=te_weight)\ndata['Breed2'] = calc_smooth_mean(data, by='Breed2', on='AdoptionSpeed', m=te_weight)\ndata['Color1'] = calc_smooth_mean(data, by='Breed1', on='AdoptionSpeed', m=te_weight)\ndata['Color2'] = calc_smooth_mean(data, by='Breed2', on='AdoptionSpeed', m=te_weight)\ndata['Color3'] = calc_smooth_mean(data, by='Breed1', on='AdoptionSpeed', m=te_weight)\ndata['State'] = calc_smooth_mean(data, by='State', on='AdoptionSpeed', m=te_weight)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf584efef4df8ae86d24bf2c8924ebef8955f0e2"
      },
      "cell_type": "code",
      "source": "data.head(2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": false,
        "trusted": true,
        "_uuid": "b1f21795e3849a6e35559eaef71215ff5832a317"
      },
      "cell_type": "code",
      "source": "do_not_use = ['Name', 'Description', 'RescuerID'] #, 'PetID'] - do not forget remove it later!\ncategorical_cols = ['Type', 'Language']\ndata = pd.get_dummies(data, columns =categorical_cols)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3eb709a7516b9bea0646012576b74b2cefe2e52"
      },
      "cell_type": "code",
      "source": "data = data.drop (do_not_use, axis =1)\ndata.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7010695d0824c962904d0e1ee4f2bacce0b55b13"
      },
      "cell_type": "code",
      "source": "X_train =data.loc[np.isfinite(data.AdoptionSpeed), :]\nX_test = data.loc[~np.isfinite(data.AdoptionSpeed), :]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "262a3239a80e571b380575df0b4f811a0218bc78"
      },
      "cell_type": "code",
      "source": "print (X_train.shape)\nprint (X_test.shape)\nX_train.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "15e866301ca302cf14427be04685ba8b225a913a"
      },
      "cell_type": "code",
      "source": "print (X_train.shape[0] == meta_train.shape[0])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86c3b21dd62e4028bc8a0ee6e1ef33c63578791a"
      },
      "cell_type": "code",
      "source": "X_test = X_test.drop(['AdoptionSpeed'], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be72851b297b0e8ce06647ce13019da3f0d694f3"
      },
      "cell_type": "code",
      "source": "print (X_test.shape[0]  == meta_test.shape[0] )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "354d4993d3b75f07b5c9ef2495144086f98fc33c"
      },
      "cell_type": "code",
      "source": "###  IMAGE FEATURES from VGG16 ###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a6275bedf5e6e599c7732cc2e65f23804d72ae6"
      },
      "cell_type": "code",
      "source": "# Modified from the kernel https://www.kaggle.com/mkozine/weighted-kappa-loss-for-keras-tensorflow\n# Eliminated bsize = (batch size), use y_pred.shape[0] instead\n\ndef kappa_loss(y_pred, y_true, y_pow=2, eps=1e-10, bsize=256, N=5, name='kappa'):\n    \"\"\"A continuous differentiable approximation of discrete kappa loss.\n        Args:\n            y_pred: 2D tensor or array, [batch_size, num_classes]\n            y_true: 2D tensor or array,[batch_size, num_classes]\n            y_pow: int,  e.g. y_pow=2\n            N: typically num_classes of the model\n                        eps: a float, prevents divide by zero\n            name: Optional scope/name for op_scope.\n        Returns:\n            A tensor with the kappa loss.\"\"\"\n\n    with tf.name_scope(name):\n        y_true = tf.to_float(y_true)\n        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n        weights = repeat_op_sq / tf.to_float((N - 1) ** 2)\n    \n        pred_ = y_pred ** y_pow\n        try:\n            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n        except Exception:\n            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n    \n        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n        hist_rater_b = tf.reduce_sum(y_true, 0)\n    \n        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n    \n        nom = tf.reduce_sum(weights * conf_mat)\n        denom = tf.reduce_sum(weights * tf.matmul(\n            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n                              tf.to_float(bsize))\n    \n        return nom / (denom + eps)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3abe6499836166cf0566dabad47f1987bac9e18d"
      },
      "cell_type": "code",
      "source": "name_target_dict = meta_train.set_index('PetID')['AdoptionSpeed'].to_dict()\n\ntrain_image_names = os.listdir(train_images_data_dir)\nn_train_images = len(train_image_names)\ntest_image_names = os.listdir(test_images_data_dir)\nn_test_images = len(test_image_names)\nprint (train_image_names [0:2])\nprint (\"No. of train images: \" + str (n_train_images))\nprint (\"No. of test images: \" + str (n_test_images))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ca1d5c78dd15fe1c1611f33f66daf6e769866ac"
      },
      "cell_type": "code",
      "source": "generator_dict = {'filename': [], 'PetID':[], 'class': []}\n\nfor name in train_image_names:\n    short_name = name.split('-')[0]\n    label = name_target_dict[short_name]\n    \n    generator_dict['filename'].append(name)\n    generator_dict['PetID'].append(short_name)\n    generator_dict['class'].append(label)\n\ngenerator_df_full = pd.DataFrame(generator_dict)\nprint (generator_df_full.shape)\ngenerator_df_full[:3]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7587be14bbac765cb288704eb3d420c24cc5e8b9"
      },
      "cell_type": "code",
      "source": "test_name_target_dict = meta_test.set_index('PetID').to_dict()\ntest_generator_dict = {'filename': [], 'PetID':[]}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "587379bf17d5ed6243bb6a792fb5fd619abfd4af"
      },
      "cell_type": "code",
      "source": "for name in test_image_names:\n    short_name = name.split('-')[0]\n    #label = test_name_target_dict[short_name]\n    \n    test_generator_dict['filename'].append(name)\n    test_generator_dict['PetID'].append(short_name)\n    \n\ntest_generator_df = pd.DataFrame(test_generator_dict)\ntest_generator_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4fe73cbe9c77ca879536e9241a75310a198b7053"
      },
      "cell_type": "code",
      "source": "len(meta_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da2bce31f54dfff52495acbc62e1eb728804ae12"
      },
      "cell_type": "code",
      "source": "np.random.seed(seed=6)\nmask = np.random.randn(len(meta_train)) < 0.9\ntrain_split = X_train[mask]\nvalidation_split = X_train[~mask]\nprint (train_split.shape)\nprint (validation_split.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01da3f9222c231fbbcefaac3eb77f615c04d2b34"
      },
      "cell_type": "code",
      "source": "# Full dicts will be helpful to attach the image features to the right PetID\ntrain_generator_full_df = generator_df_full.loc[generator_df_full['PetID'].isin(train_split['PetID'].values)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c641f9b772c8ff4ed7f6df25ebc37e672ce3f763"
      },
      "cell_type": "code",
      "source": "train_generator_df = train_generator_full_df.copy()\ntrain_generator_df = train_generator_df[['filename','class']]\ntrain_generator_df['class'] = train_generator_df['class'].astype(str)\ntrain_generator_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b965a1ecdcc1dd50bc0f34fbeae61d974265f69"
      },
      "cell_type": "code",
      "source": "valid_generator_full_df = generator_df_full.loc[generator_df_full['PetID'].isin(validation_split['PetID'].values)]\nvalid_generator_df = valid_generator_full_df.copy()\nvalid_generator_df = valid_generator_df[['filename','class']]\nvalid_generator_df['class'] = valid_generator_df['class'].astype(str)\nvalid_generator_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4db78596311649d68359a8dca1dccd1244ab2a0d"
      },
      "cell_type": "code",
      "source": "import keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Flatten, Dense, BatchNormalization\nfrom keras import applications",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d460b69bf1f7228b6582ddcb72c1c07041e2e38c"
      },
      "cell_type": "code",
      "source": "# dimensions of our images.\nimg_width, img_height = 150, 150\nepochs = 50\nbatch_size = 32",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d44c01559cc2cbe01e326bdbcd2133c4f937b8f"
      },
      "cell_type": "code",
      "source": "datagen = ImageDataGenerator( rescale=1/255.)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "253d560eec7f3f7bb18ba002dcc3dccd778a0992"
      },
      "cell_type": "code",
      "source": "# Create data generator for the VGG16 part of the model\ndef create_generator_vgg16(data_dir, input_df):\n    return datagen.flow_from_dataframe(\n        input_df, \n        data_dir, \n        x_col='filename',\n        y_col='class', \n        has_ext=True,  # If image extension is given in x_col\n        target_size=(img_width, img_height), \n        color_mode='rgb',\n        class_mode=None, \n        batch_size=batch_size, \n        shuffle=False, # we will just apply the fixed VGG16 weights to the images \n        seed=6\n    )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c56eda6c1f8c8dc8224fac4c169e0f1d87ba87d"
      },
      "cell_type": "code",
      "source": "# Make length of train and valid splits divisible by batch_size\n# for the future VGG-16 propagation needs\nn_train_split = train_generator_df.shape[0]//batch_size*batch_size\nprint (\" Length of train portion decreased from \" + str(train_generator_df.shape[0]) + \" to \" \n       + str (n_train_split))\nn_valid_split = valid_generator_df.shape[0]//batch_size*batch_size\nprint (\" Length of valid portion decreased from \" + str(valid_generator_df.shape[0]) + \" to \" \n       + str (n_valid_split))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c36c248e51477d6374d5ebeb7137e96cf27240d"
      },
      "cell_type": "code",
      "source": "train_generator_df_short = train_generator_df.head(n_train_split)\ntrain_generator_full_df_short = train_generator_full_df.head(n_train_split)\ntrain_generator_df_short.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed9e25dc9a02962556eabbb80e532a1d359cbfea"
      },
      "cell_type": "code",
      "source": "valid_generator_full_df_short = valid_generator_full_df.head(n_valid_split)\nvalid_generator_df_short = valid_generator_df.head(n_valid_split)\nvalid_generator_df_short.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07e3ec95f2231275512834929a2c57365bb628b2"
      },
      "cell_type": "code",
      "source": "train_generator = create_generator_vgg16(train_images_data_dir, train_generator_df_short)\nvalid_generator = create_generator_vgg16(train_images_data_dir, valid_generator_df_short)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1428dfe98ae5cf596fba5bfff367035a768f7a39"
      },
      "cell_type": "code",
      "source": "from keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nbase_model = VGG16(include_top = False,\n                  input_shape=(img_width, img_height,3),\n                  weights='../input/vgg16-weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f8a89219e7da8341b22295856a1ada71441dcd2"
      },
      "cell_type": "code",
      "source": "bottleneck_features_train = base_model.predict_generator(train_generator, n_train_split//batch_size, verbose = 1)\nbottleneck_features_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ab54f14223b4ba8a25a4ac3dc4db416ee42a13d"
      },
      "cell_type": "code",
      "source": "bottleneck_features_valid = base_model.predict_generator(valid_generator, n_valid_split // batch_size, verbose = 1)\nbottleneck_features_valid.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7892539652591cc613e5eff5d4fb029a39f4ee3b"
      },
      "cell_type": "code",
      "source": "from keras.utils import to_categorical\ntrain_labels = to_categorical(train_generator_df_short['class'])\nvalid_labels = to_categorical(valid_generator_df_short['class'])\nvalid_labels.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b55c4f393f7ee510e6b42321040592991ca92194"
      },
      "cell_type": "code",
      "source": "# build dense layesr model on to of VGG16\ntop_model = Sequential()\ntop_model.add(Flatten(input_shape=bottleneck_features_train.shape[1:]))\ntop_model.add(Dense(256, activation='relu'))\ntop_model.add(Dropout(0.6))\ntop_model.add(BatchNormalization (epsilon=0.001))\n#top_model.add(Dense(16, activation='relu'))\n#top_model.add(Dropout(0.4))\n#top_model.add(BatchNormalization (epsilon=0.001))\ntop_model.add(Dense(5, activation='softmax'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4cdf7bac5e3e15424dd78531ce201311b404236"
      },
      "cell_type": "code",
      "source": "#top_model.compile(optimizer='rmsprop',\n#              loss='binary_crossentropy', metrics=['accuracy'])\nmko_optimizer = keras.optimizers.rmsprop(lr=0.00005)\ntop_model.compile(loss = kappa_loss,\n              optimizer = mko_optimizer,\n              metrics=['accuracy'])\n\ntop_model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd9674e76acfff288c6aa47f7e9f06f688f35727"
      },
      "cell_type": "code",
      "source": "top_model.fit(bottleneck_features_train, train_labels,\n          epochs=30,\n          batch_size=32,\n          validation_data=(bottleneck_features_valid, valid_labels))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "526ba17b1371f50ff19ca7b463247d574e62102e"
      },
      "cell_type": "code",
      "source": "# Model is ready.\n# Use it to create image features for train, valid and test portions\n# First - train\ntrain_images_predictions = top_model.predict(bottleneck_features_train, verbose=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ceb558e39b3efec0e5c9f6bbe26aba579d7bfbf"
      },
      "cell_type": "code",
      "source": "train_generator_full_df_short.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba3b3601d34bda96fb7d4dcc70297c008dce9e0a"
      },
      "cell_type": "code",
      "source": "tr_df = pd.DataFrame(train_images_predictions, columns = (\"Img_0\", \"Img_1\",\"Img_2\",\"Img_3\",\"Img_4\"))\ntr_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e83d1c5ca48f61463e5659121ec4f5290f4e2526"
      },
      "cell_type": "code",
      "source": "# Pack it nicely and merge\ntrain_images_df = train_generator_full_df_short.join(pd.DataFrame(train_images_predictions, columns = (\"Img_0\", \"Img_1\",\"Img_2\",\"Img_3\",\"Img_4\")))\ntrain_images_df.drop(columns=['filename', 'class'], inplace=True)\ntrain_images_df.loc[:,'Img_pred'] = train_images_df.iloc[:,1:6].values.argmax(axis=1)\nprint(train_images_df.shape)\ntrain_images_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62a0cfa79c5971c0d72fa674a7031f30d8e6fac7"
      },
      "cell_type": "code",
      "source": "train_images_df = train_images_df.groupby('PetID', as_index=False).median()\ntrain_images_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60d045506f61684adc87f873f373505626a7c476"
      },
      "cell_type": "code",
      "source": "train_split.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9f09afaf13522922e86c4ffbba7378086040e98"
      },
      "cell_type": "code",
      "source": "#train_images_df = train_images_df.groupby('PetID', as_index=False).mean()\ntrain_split = pd.merge(train_split, train_images_df, how='left', on = 'PetID')\ntrain_split.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be4671b2c0a90cc953dc9fa98a57968545c31493"
      },
      "cell_type": "code",
      "source": "train_split.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7841344afd4b489ac07021dc3f520c73075a75c6"
      },
      "cell_type": "code",
      "source": "'''train_split['Img_0'] = train_split.Img_0.fillna(0)\ntrain_split['Img_1'] = train_split.Img_1.fillna(0)\ntrain_split['Img_2'] = train_split.Img_2.fillna(0)\ntrain_split['Img_3'] = train_split.Img_3.fillna(0)\ntrain_split['Img_4'] = train_split.Img_4.fillna(0)'''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a071fe88211ee7deb74bc0ba5fbf673e9d011ca"
      },
      "cell_type": "code",
      "source": "train_split['Img_pred'][train_split['Img_pred']==0.000000] = float('nan')\n#train_split['Img_pred'] = train_split.Img_pred.fillna(3)\ntrain_split[['AdoptionSpeed','Img_pred']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20f0bd94372f4ccf915b44d79d6b88df74c2a022"
      },
      "cell_type": "code",
      "source": "# Valid\nvalid_images_predictions = top_model.predict(bottleneck_features_valid, verbose=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d8db7545f2d6b563ee2ae2fb9dd8dd60d6d4a05"
      },
      "cell_type": "code",
      "source": "valid_images_df = valid_generator_full_df_short.join(pd.DataFrame(valid_images_predictions, columns = (\"Img_0\", \"Img_1\",\"Img_2\",\"Img_3\",\"Img_4\")))\nvalid_images_df.drop(columns=['filename', 'class'], inplace=True)\nvalid_images_df['Img_pred'] = valid_images_df.iloc[:,1:6].values.argmax(axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4529992954d03ce3273bd47ade78e600bb116f44"
      },
      "cell_type": "code",
      "source": "valid_images_df = valid_images_df.groupby('PetID', as_index=False).mean()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90e38ed24f90c639f78b2c23409a81246b96d5cf"
      },
      "cell_type": "code",
      "source": "valid_images_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c2dcc5507ad7b89fdf8518c44dbb480bba9791e"
      },
      "cell_type": "code",
      "source": "validation_split = pd.merge(validation_split, valid_images_df, how='left', on='PetID')\nprint (validation_split.shape)\nvalidation_split.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb1187a74ec84ae1d27925e18db87796f3c753c6"
      },
      "cell_type": "code",
      "source": "validation_split['Img_pred'][validation_split['Img_pred']==0.000000] = float ('nan')\n#validation_split['Img_pred'] = validation_split.Img_pred.fillna(3)\n#validation_split['Img_0'] = validation_split.Img_0.fillna(0)\n#validation_split['Img_1'] = validation_split.Img_1.fillna(0)\n#validation_split['Img_2'] = validation_split.Img_2.fillna(0)\n#validation_split['Img_3'] = validation_split.Img_3.fillna(0)\n#validation_split['Img_4'] = validation_split.Img_4.fillna(0)\n#validation_split[['AdoptionSpeed','Img_pred']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cd3b58be20625f76e6dc0c21aa1b6ede2e4141af"
      },
      "cell_type": "code",
      "source": "X_train1 = train_split.append(validation_split)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf70cdde72cbcf6af4feb74232fc4c7821fc0dcd"
      },
      "cell_type": "code",
      "source": "X_train1.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8675cac779346b16e358bfad573b07363aa8ee04"
      },
      "cell_type": "code",
      "source": "X_train1[:5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74082fa6a3a21b098970c3e89aedf8749485ecbb"
      },
      "cell_type": "code",
      "source": "# Test\ntest_generator = ImageDataGenerator(rescale=1/255.).flow_from_dataframe(\n    test_generator_df,\n    test_images_data_dir,\n    has_ext=True,\n    target_size=(img_width, img_height),\n    color_mode='rgb',\n    batch_size=64,\n    shuffle=False,\n    class_mode=None\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e1cd901ab6eaa6b74b8c910a8f01033b181fd9c"
      },
      "cell_type": "code",
      "source": "bottleneck_features_test = base_model.predict_generator(test_generator, len(test_generator), verbose=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af7751d68e989d2cbaa20ed4e940c7a161b25574"
      },
      "cell_type": "code",
      "source": "test_images_predictions = top_model.predict(bottleneck_features_test, verbose=1 )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20466a0695741310caccedf5b8b3826c393e2c5c"
      },
      "cell_type": "code",
      "source": "test_images_df = test_generator_df.join(pd.DataFrame(test_images_predictions, columns = (\"Img_0\", \"Img_1\",\"Img_2\",\"Img_3\",\"Img_4\")))\ntest_images_df.drop(columns=['filename'], inplace=True)\nprint(test_images_df.shape)\ntest_images_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31a6b0f533b1f1a994cf339e294f0592e83d00b1"
      },
      "cell_type": "code",
      "source": "#data.loc[:, 'NameWordLength'] =\ntest_images_df.loc[:,'Img_pred'] = test_images_df.iloc[:,1:6].values.argmax(axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d42d2521c42ac55637fba9a587d99a5fd0ba6925"
      },
      "cell_type": "code",
      "source": "test_images_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c2c7dc32864d5bd55ed640aeabcadf0d1423e40"
      },
      "cell_type": "code",
      "source": "test_images_df = test_images_df.groupby('PetID', as_index=False).mean()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ffd1f4d5c35122529fb2ac844a3540db802cb9a6"
      },
      "cell_type": "code",
      "source": "print (X_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7145b180017f395c440596252145cd80f43a6097"
      },
      "cell_type": "code",
      "source": "print (X_test.shape)\nX_test = pd.merge(X_test, test_images_df, how='left', on = 'PetID')\nprint (X_test.shape)\nX_test.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d3a48a4ba8dac0550dedb61192a3448788c04cf"
      },
      "cell_type": "code",
      "source": "X_test['Img_pred'][X_test['Img_pred']==0.000000] = float('nan')\n#X_test['Img_pred'] = X_test.Img_pred.fillna(0)\n#X_test['Img_0'] = X_test.Img_0.fillna(0)\n#X_test['Img_1'] = X_test.Img_1.fillna(0)\n#X_test['Img_2'] = X_test.Img_2.fillna(0)\n#X_test['Img_3'] = X_test.Img_3.fillna(0)\n#X_test['Img_4'] = X_test.Img_4.fillna(0)\nX_test.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58af24724ac99d14bdc8e3948bd684b5b06c0ec6"
      },
      "cell_type": "code",
      "source": "X_train1 = X_train1.drop('PetID', axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab127720661de0cfeba9f8eec1b9056df18d6a1c"
      },
      "cell_type": "code",
      "source": "X_test = X_test.drop('PetID', axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "268f11cf0ea597536230d04f1be68fe186367eaf"
      },
      "cell_type": "code",
      "source": "#######################################################################################################",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e43872be03a8e2bb4b0ab35b78d41f655d952f0"
      },
      "cell_type": "code",
      "source": "#### End IMAGE FEATURES from VGG16 ###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d65a556946f7ec0e6e94548e7a7515e011e83733"
      },
      "cell_type": "code",
      "source": "###### BEGIN FEATURES FOR RESNET ######",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef5004a2af29a9419f6f47c1a36e4e509a70e9a5"
      },
      "cell_type": "code",
      "source": "def resnet_script():\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n    import matplotlib.pyplot as plt\n    import os\n    from os.path import isfile, join, abspath, exists, isdir, expanduser\n\n    from functools import reduce\n    from shutil import copyfile\n    import torchvision.transforms as transforms\n\n    from skimage import io, transform, color\n    from timeit import default_timer as timer\n    from PIL import Image\n\n    import torch\n    from torch import nn # import neural network\n    from torch import optim # import optimization\n    import torch.nn.functional as F\n    from torchvision import datasets, transforms, models\n    from collections import OrderedDict\n    from torch.utils import data\n\n    input_dir = '../input/petfinder-adoption-prediction/'\n\n    train = pd.read_csv(input_dir + 'train/train.csv')\n    test = pd.read_csv(input_dir + 'test/test.csv')\n\n    def get_photo_file_names(PetID, PhotoAmt):\n        return ['{}-{}'.format(PetID, str(num+1)) for num in range(int(PhotoAmt))]\n\n    train['PhotoFileNames'] = train.apply(lambda row: get_photo_file_names(row['PetID'], row['PhotoAmt']), axis=1)\n    test['PhotoFileNames'] = test.apply(lambda row: get_photo_file_names(row['PetID'], row['PhotoAmt']), axis = 1)\n\n    ## create training and validation split \n    mask = np.random.randn(len(train))<0.8\n    train_split = train[mask]\n    validation_split = train[~mask]\n\n    partition = {'train': reduce(lambda x,y: x+y, train_split.PhotoFileNames),\n             'validation': reduce(lambda x,y: x+y, validation_split.PhotoFileNames),\n             'test' : reduce(lambda x,y: x+y, test.PhotoFileNames)}\n\n    # create labels dictionary for all the images\n    def get_photo_label_dict(PhotoFileNames, Type):\n        return dict(zip(PhotoFileNames, np.repeat(Type, len(PhotoFileNames))))\n\n    labels = {}\n    photo_label_dict_list = train.apply(lambda row: get_photo_label_dict(row['PhotoFileNames'], row['AdoptionSpeed']), axis=1).values\n\n    for photo_label_dict in photo_label_dict_list:\n        labels.update(photo_label_dict)\n\n    class AdoptionDataset(data.Dataset):\n        def __init__(self, list_IDs, labels=None, transform=None, train_split=True):\n            self.train_split = train_split\n            self.transform = transform\n            self.labels = labels\n            self.list_IDs = list_IDs\n\n        def __len__(self):\n            return len(self.list_IDs)\n\n        def __getitem__(self, index):\n            # Select sample\n            ID = self.list_IDs[index]\n            split_type = ['train' if self.train_split else 'test'][0]\n            image_path = '{input_dir}{split_type}_images/{ID}.jpg'.format(input_dir=input_dir, split_type=split_type, ID=ID)\n\n            # Load data and get label\n            image = io.imread(image_path)\n            image = color.gray2rgb(image)\n        #         label = self.labels[ID]\n\n            if self.train_split:\n                label = self.labels[ID]\n                if self.transform:\n                    image = self.transform(image)\n                    label = torch.tensor(label)\n                return image, label\n            else:\n                if self.transform:\n                    image = self.transform(image)\n                return image\n\n    params = {'batch_size': 64,\n          'shuffle': True,\n          }\n\n    train_transforms = transforms.Compose([transforms.ToPILImage(),\n                                       transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n\n    vali_test_transforms = transforms.Compose([transforms.ToPILImage(),\n                                      transforms.Resize(256),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n\n\n    input_size = 224\n\n    training_set = AdoptionDataset(partition['train'], labels, transform=train_transforms)\n    training_generator = data.DataLoader(training_set, **params)\n\n    validation_set = AdoptionDataset(partition['validation'], labels, transform=vali_test_transforms)\n    validation_generator = data.DataLoader(validation_set, **params)\n\n    test_set = AdoptionDataset(partition['test'], transform=vali_test_transforms, train_split=False)\n    test_generator = data.DataLoader(test_set, batch_size = 1, shuffle=False)\n\n    class_to_idx = dict({'0': 0, '1': 1, '2': 2, '3': 3, '4': 4})\n\n    trainiter = iter(training_generator)\n    features, labels = next(trainiter)\n\n    valiter = iter(validation_generator)\n    features, labels = next(valiter)\n\n    testiter = iter(test_generator)\n    features = next(testiter)\n\n    def imshow(image, ax=None, title=None, normalize=True):\n        \"\"\"Imshow for Tensor.\"\"\"\n        if ax is None:\n            fig, ax = plt.subplots()\n        image = image.numpy().transpose((1, 2, 0))\n\n        # because in the transform above, we normalized the image\n        # so we need to convert back to original\n        if normalize:\n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            image = std * image + mean\n            image = np.clip(image, 0, 1)\n\n        ax.imshow(image)\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        ax.spines['bottom'].set_visible(False)\n        ax.tick_params(axis='both', length=0)\n        ax.set_xticklabels('')\n        ax.set_yticklabels('')\n\n        return ax\n\n    model = models.resnet152()\n    model.load_state_dict(torch.load(\"../input/resnet152/resnet152.pth\"))    \n\n    drop_p = 0.3\n    learning_rate = 0.001\n    n_inputs = model.fc.in_features\n    num_classes = 5 \n\n\n    # Add on classifier\n    model.fc = nn.Sequential(nn.Linear(n_inputs, 1000),\n                         nn.ReLU(),\n                         nn.Dropout(drop_p),\n                         nn.BatchNorm1d(1000),\n                         nn.Linear(1000, num_classes),\n                         nn.LogSoftmax(dim=1))\n    # define criterion and optimizer\n    criterion = nn.NLLLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = learning_rate)    \n    # Freeze model weights\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Unfreeze some layers\n\n    for i in range(1,7):\n        list(model.parameters())[-i].requires_grad = True\n\n    def validation(model, criterion, dataset):\n\n        model.to(device)\n\n        accuracy = 0\n        test_loss = 0\n\n        for inputs, labels in iter(dataset):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            output = model.forward(inputs)\n            test_loss += criterion(output, labels).item()\n\n            ## Calculating the accuracy\n            # Model's output is log-softmax, take exponential to get the probabilities\n            ps = torch.exp(output)\n\n            # Class with highest probability is our predicted class, compare with true label\n            equality = (labels.data == ps.max(1)[1])\n\n            # Accuracy is number of correct predictions divided by all predictions, just take the mean\n            accuracy += equality.type_as(torch.FloatTensor()).mean()\n\n        return test_loss, accuracy\n\n    # train the network\n    epochs = 1\n    print_every = 40\n    steps = 0\n    model.train()\n    # change to cuda if avaliable\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    for e in range(epochs):\n        running_loss = 0\n\n    for inputs, labels in iter(training_generator):\n        steps +=1\n\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        # Forward and backward passes\n        outputs = model.forward(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        if steps % print_every == 0:\n\n            # Model in inference mode, dropout is off\n            model.eval()\n\n            # Turn off gradients for validation, will speed up inference\n            with torch.no_grad():\n                test_loss, accuracy = validation(model, criterion, dataset = validation_generator, )\n\n            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n                  \"Validation Loss: {:.3f}.. \".format(test_loss/len(validation_generator)),\n                  \"Validation Accuracy: {:.3f}\".format(accuracy/len(validation_generator)))\n\n            running_loss = 0\n\n            # Make sure dropout and grads are on for training\n            model.train()    \n\n    def predict(img,model,k = 1):\n        img = img.to(torch.device(device))\n        # convert to tensor with the right format \n        img = img.unsqueeze(0) \n\n        # move img2 to cuda \n        img2 = img\n\n        #put the image to the model for prediction\n        with torch.no_grad():\n            output = model.forward(img2)\n\n        # get probabilities and classes\n        probs, classes = output.topk(k)\n\n        # model is in logsoftmax, use exp() to convert back to probabilities\n        # since the format is Tensor, convert back to numpy array\n        ps = torch.exp(probs)\n        probs = ps[0]\n        classes = classes[0]\n\n        # print(probs)\n        # print(classes)\n\n        return classes[0], torch.exp(output)    \n\n    # put the model in eval mode\n    model.eval()\n\n    # change to cuda if avaliable\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    classes, output = predict(features[0],model, k=5)\n    #print('Probability is: ', probs)\n    print('classes is: ', classes)    \n\n    test_predictions = {}\n    test_output = {}\n    for filename, images in zip(partition['test'], test_generator):\n        classes, output = predict(images[0], model, k = 5)   \n        test_predictions[filename] = classes\n        test_output[filename] = output.cpu()\n\n    test_submissions_output = pd.DataFrame((k, *v[0]) for k, v in test_output.items())    \n    \n    for i in range(1,6):\n        test_submissions_output[i] = test_submissions_output[i].astype(float)\n\n    test_submissions_output.columns = ['PetID', 'ResNet-0', 'ResNet-1', 'ResNet-2', 'ResNet-3', 'ResNet-4']\n    test_submissions_output['PetID'] = test_submissions_output['PetID'].apply(lambda x: x[:-2])    \n    test_submissions_output = test_submissions_output.groupby('PetID').mean().reset_index()\n    test_submissions = pd.DataFrame(dict([ (k,pd.Series(v.tolist())) for k,v in test_predictions.items() ])).melt()\n\n    test_submissions.rename(columns={'variable':'FileName', 'value': 'AdoptionSpeed_ResNet'}, inplace=True)\n    test_submissions['PetID'] = test_submissions.FileName.apply(lambda x: x[:-2])\n\n    test_submissions = test_submissions[['PetID','AdoptionSpeed_ResNet']].groupby('PetID').min().reset_index()\n    #test_submissions = test_submissions[['PetID','AdoptionSpeed']].groupby('PetID').mean().reset_index()    \n    testPetID = test.PetID.to_frame()\n    final = testPetID.merge(test_submissions,how = 'left',on = 'PetID').fillna(3)\n    final['AdoptionSpeed_ResNet']=final['AdoptionSpeed_ResNet'].astype(int)    \n\n    # Merge this and use as XGBoost features\n    final = pd.merge(final, test_submissions_output, on='PetID', how='left')\n\n    return final\n\nresnet_features = resnet_script()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c9f016d0a3b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m \u001b[0mresnet_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-c9f016d0a3b2>\u001b[0m in \u001b[0;36mresnet_script\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c9f016d0a3b2>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Load data and get label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray2rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#         label = self.labels[ID]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, flatten, **plugin_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                                (plugin, kind))\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, dtype, img_num, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mpil_to_ndarray\u001b[0;34m(image, dtype, img_num)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# this will raise an IOError if the file is not readable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://pillow.readthedocs.org/en/latest/installation.html#external-libraries\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mgetdata\u001b[0;34m(self, band)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \"\"\"\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mband\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetband\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mband\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3296ab279e81e83cf40ef990daad621a642e9a7d"
      },
      "cell_type": "code",
      "source": "###### END RESNET ######",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a608b1abf03f64952331250264822eaf015437f"
      },
      "cell_type": "code",
      "source": "#X_test = X_test.drop('PetID', axis = 1)\n#X_train = X_train.drop('PetID', axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c7c81c9871d8b88367faeab99f7df282f26828f4"
      },
      "cell_type": "code",
      "source": "#X_train_non_null = X_train1.fillna(-1)\nX_train_non_null = X_train1.fillna(-1)\nX_test_non_null = X_test.fillna(-1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a396095e7d8de68c3b1de3ba7660912cf994f609"
      },
      "cell_type": "code",
      "source": "X_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b978e9ed698552487f55f2ea9fc56ec72e1e75c1"
      },
      "cell_type": "code",
      "source": "X_train_non_null.shape, X_test_non_null.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3267dc181ba0606a1b52f942cb6437445ce991b1"
      },
      "cell_type": "code",
      "source": "len(X_train.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "230343dc23e8061cb1b320b5e82be07b48c01e51"
      },
      "cell_type": "code",
      "source": "import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43b66e49fcc112873182fbb17f42fd0052a93afa"
      },
      "cell_type": "code",
      "source": "# Quadratic Kappa calculation\n\n# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94209bc640fc0559cdc350763be903fc855af4dd"
      },
      "cell_type": "code",
      "source": "# Optimal split thresholds\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6e6e4b9dc7c2903ff3a6ed7e423575eb5e052a6"
      },
      "cell_type": "code",
      "source": "# XGBoost\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\nxgb_params = {\n    'eval_metric': 'rmse',\n    'seed': 6,\n    'eta': 0.001,\n    'gamma': 2,\n    'max_depth': 8,\n    #'predictor': 'gpu_predictor',\n    'subsample': 0.8,\n    'colsample_bytree': 0.85,\n    #'tree_method': 'gpu_hist',\n    #'device': 'gpu',\n    'silent': 1,\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "849b8a8a898a12b5e18dfd145cb73105a44011b2"
      },
      "cell_type": "code",
      "source": "def run_xgb(params, X_train, X_test):\n    n_splits = 10\n    verbose_eval = 1000\n    num_rounds = 60000\n    early_stop = 500\n\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n\n    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n\n        i += 1\n    return model, oof_train, oof_test\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b55a2df1bcd155eb38a1ef3afe711f822fc0c95f"
      },
      "cell_type": "code",
      "source": "#model, oof_train, oof_test = run_xgb(xgb_params, X_train1, X_test)\nmodel, oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3de02eaa53a182118935f248fda0351493ce4e91"
      },
      "cell_type": "code",
      "source": "def plot_pred(pred):\n    sns.distplot(pred, kde=True, hist_kws={'range': [0, 5]})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84c387050ca7ba65a25fec73590bd22b0906230a"
      },
      "cell_type": "code",
      "source": "plot_pred(oof_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "15e0ff7159cbc19f7d109f6aee88cff62c4e7c66"
      },
      "cell_type": "code",
      "source": "plot_pred(oof_test.mean(axis=1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7591e632e3b6ac23f51fc230d09604c3de50d8b2"
      },
      "cell_type": "code",
      "source": "optR = OptimizedRounder()\noptR.fit(oof_train, X_train1['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(oof_train, coefficients)\nqwk = quadratic_weighted_kappa(X_train1['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK = \", qwk)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4114f42c8d94fad5976c7d23911843ad54d0fb30"
      },
      "cell_type": "code",
      "source": "coefficients",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b84a1eb787b76cf0e8a52f5754d945964bfc7be0"
      },
      "cell_type": "code",
      "source": "#coef = [1.66,2.13,2.47522154,2.85]\nttt = pd.cut(valid_pred, [-np.inf] + list(np.sort(coefficients)) + [np.inf], labels = [0, 1, 2, 3, 4])\nquadratic_weighted_kappa(X_train1['AdoptionSpeed'].values, ttt)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60cb79560931d17308161cf4816f4e6b5760c88d"
      },
      "cell_type": "code",
      "source": "train_predictions = optR.predict(oof_train, coefficients).astype(np.int8)\nprint(f'train pred distribution: {Counter(train_predictions)}')\ntest_predictions = optR.predict(oof_test.mean(axis=1), coefficients).astype(np.int8)\nprint(f'test pred distribution: {Counter(test_predictions)}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "613398bfd20671b2cc4f07ed913617729e959d50"
      },
      "cell_type": "code",
      "source": "coefficients_ = coefficients.copy()\ncoefficients_[0] = 1.66\ncoefficients_[1] = 2.13\ncoefficients_[3] = 2.85\ntrain_predictions = optR.predict(oof_train, coefficients_).astype(np.int8)\nprint(f'train pred distribution: {Counter(train_predictions)}')\ntest_predictions = optR.predict(oof_test.mean(axis=1), coefficients_).astype(np.int8)\nprint(f'test pred distribution: {Counter(test_predictions)}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ae24a1ccd7980f6a335c9e0618fd05168c32f65"
      },
      "cell_type": "code",
      "source": "ttt = pd.cut(valid_pred, [-np.inf] + list(np.sort(coefficients_)) + [np.inf], labels = [0, 1, 2, 3, 4])\nquadratic_weighted_kappa(X_train1['AdoptionSpeed'].values, ttt)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a138a84a800aabbd1398c6494e7ad935546b83fa"
      },
      "cell_type": "code",
      "source": "# Submission\n#test_PetID = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv').PetID\n#submission = pd.DataFrame({'PetID':test_PetID, 'AdoptionSpeed':test_predictions})\nsubmission = pd.DataFrame({'PetID': meta_test['PetID'].values, 'AdoptionSpeed': test_predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6023ecb8eae9cbddb551308461dc1976bc1ec359"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2af4dfec74dcbf6ada2db57569bdcfb19551295"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}